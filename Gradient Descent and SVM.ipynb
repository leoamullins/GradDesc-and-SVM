{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04cda3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16827825",
   "metadata": {},
   "source": [
    "# Part 1 - Gradient descent and ascent\n",
    "\n",
    "Gradient ascent and descent algorithms for find local minima and maxima of a differentiable function of several variables. This code will implement gradient descent for functions $ F : \\mathbb{R}^2 \\to \\mathbb{R} $ and create plots that demonstrate the behaviour.\n",
    "\n",
    "First we define the **gradient** of a function $ F : \\mathbb{R}^2 \\to \\mathbb{R} $, written $ \\nabla F$, is the function $ \\nabla F : \\mathbb{R}^2 \\to \\mathbb{R}$ given by\n",
    "\n",
    "$$\n",
    "\\nabla F(\\mathbf{v}) = \\begin{pmatrix} \\frac{\\partial F}{\\partial x}(\\mathbf{v}) \\\\ \\frac{\\partial F}{\\partial y}(\\mathbf{v})\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where $\\frac{\\partial F}{\\partial x}(\\mathbf{v})$ is the partial derivative defined by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial F}{\\partial x}(v_{1}, v_{2}) = \\lim_{h \\to 0}\\frac{F(v_1 + h, v_{2}) - F(v_1, v_{2})}{h}\n",
    "$$\n",
    "\n",
    "and $ \\frac{\\partial F}{\\partial y} $ is defined similarly.\n",
    "\n",
    "The significance of the gradient is that at a point $\\mathbf{v} \\in \\mathbb{R}^2$, the direction in which $F$ increases fastest is $\\nabla F(\\mathbf{v})$ and the direction in which $F$ decreases fastest is $ - \\nabla F(\\mathbf{v})$.\n",
    "\n",
    "The gradient gives us a simple way of looking for a local maximum of a function $F: \\mathbb{R}^2 \\to \\mathbb{R}$. Pick some point $\\mathbf{a}_0$ to start off at, then move a small distance in the direction of $\\nabla F( \\mathbf{a}_0)$, taking you to a new point $\\mathbf{a}_1$ with a bigger value of $F$, then move a small distance in the direction of $\\nabla F(\\mathbf{a}_1)$ to a new point $\\mathbf{a}_2$, and so on.  This is gradient ascent.  (To see how the method works, imagine you are standing somewhere on a foggy mountain and want to climb to the top.  If you keep walking in the steepest uphill direction you can find, you may eventually get to the top, as long as there is only one peak).  To find a local minimum value of $F$ we can do the same thing except we move in the direction $- \\nabla F$, since this is the direction in which $F$ decreases fastest - this is gradient *de*scent.\n",
    "\n",
    "In this code, an approximation of the gradient function is used\n",
    "\n",
    "$$\\frac{f(a+h, b) - f(a, b)}{h} $$\n",
    "\n",
    "which is an approximation of $\\frac{\\partial f}{\\partial x}(a, b)$. The partial derivative with respect to $y$ is defined similarly for some small $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ada852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_grad(f, v, h):\n",
    "    x, y = v\n",
    "\n",
    "    x_partial = (f(x+h, y) - f(x, y)) / h\n",
    "    y_partial = (f(x, y+h) - f(x, y)) / h\n",
    "\n",
    "    return x_partial, y_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a605c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the gradient function with a simple function\n",
    "def f(x, y):\n",
    "    return x * y + x ** 2 + 2 * y + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2949b698",
   "metadata": {},
   "source": [
    "The gradient descent algorithm for a function $ f : \\mathbb{R} ^ 2 \\to \\mathbb{R}$ begins with a point $ \\mathbf{a}_0 \\in \\mathbb{R} ^ 2$ and produced a sequence defined by\n",
    "\n",
    "$$\n",
    "\\mathbf{a}_{i+1} \\approx \\mathbf{a}_i - k \\nabla f(\\mathbf{a}_i)\n",
    "$$\n",
    "\n",
    "where $k$ is a fixed small positive number called the *step* size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bd7784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_sequence(f, a0, k, n):\n",
    "    a = np.array(a0)\n",
    "    sequence = [a.copy()]\n",
    "    for i in range(n):\n",
    "        grad = approx_grad(f, a, k)\n",
    "        a = a - k * np.array(grad)\n",
    "        sequence.append(a.copy())\n",
    "    return sequence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
